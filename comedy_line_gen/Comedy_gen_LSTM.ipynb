{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a3a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the text files \n",
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"funny_text-20230721T121912Z-001.zip\", 'r')\n",
    "files.extractall('dataset')\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b457b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\geniu\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\geniu\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b49fbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AJ_TP_text_01.txt',\n",
       " 'AJ_TP_text_02.txt',\n",
       " 'AJ_TP_text_03.txt',\n",
       " 'AJ_TP_text_04.txt',\n",
       " 'AJ_TP_text_05.txt',\n",
       " 'AJ_TP_text_06.txt',\n",
       " 'AJ_TP_text_07.txt',\n",
       " 'AJ_TP_text_08.txt',\n",
       " 'AJ_TP_text_09.txt',\n",
       " 'AJ_TP_text_10.txt',\n",
       " 'AJ_TP_text_11.txt',\n",
       " 'AJ_TP_text_12.txt',\n",
       " 'AJ_TP_text_13.txt',\n",
       " 'AJ_TP_text_14.txt',\n",
       " 'AJ_TP_text_15.txt',\n",
       " 'AJ_TP_text_16.txt',\n",
       " 'AJ_TP_text_17.txt',\n",
       " 'AJ_TP_text_18.txt',\n",
       " 'AJ_TP_text_19.txt',\n",
       " 'AJ_TP_text_20.txt',\n",
       " 'AJ_TP_text_21.txt',\n",
       " 'AJ_TP_text_22.txt',\n",
       " 'AJ_TP_text_23.txt',\n",
       " 'AJ_TP_text_24.txt',\n",
       " 'AJ_TP_text_25.txt',\n",
       " 'AJ_TP_text_26.txt',\n",
       " 'AJ_TP_text_27.txt',\n",
       " 'AJ_TP_text_28.txt',\n",
       " 'AJ_TP_text_29.txt',\n",
       " 'AJ_TP_text_30.txt',\n",
       " 'AJ_TP_text_31.txt',\n",
       " 'AJ_TP_text_32.txt',\n",
       " 'AJ_TP_text_33.txt',\n",
       " 'AW_BC_text_01.txt',\n",
       " 'AW_BC_text_02.txt',\n",
       " 'AW_BC_text_03.txt',\n",
       " 'AW_BC_text_04.txt',\n",
       " 'AW_BC_text_05.txt',\n",
       " 'AW_BC_text_06.txt',\n",
       " 'AW_BC_text_07.txt',\n",
       " 'AW_BC_text_08.txt',\n",
       " 'AW_BC_text_09.txt',\n",
       " 'AW_BC_text_10.txt',\n",
       " 'AW_BC_text_11.txt',\n",
       " 'AW_BC_text_12.txt',\n",
       " 'AW_BC_text_13.txt',\n",
       " 'AW_BC_text_14.txt',\n",
       " 'AW_BC_text_15.txt',\n",
       " 'AW_BC_text_16.txt',\n",
       " 'AW_BC_text_17.txt',\n",
       " 'AW_BC_text_18.txt',\n",
       " 'AW_BC_text_19.txt',\n",
       " 'AW_BC_text_20.txt',\n",
       " 'AW_BC_text_21.txt',\n",
       " 'AW_BC_text_22.txt',\n",
       " 'AW_BC_text_23.txt',\n",
       " 'AW_BC_text_24.txt',\n",
       " 'AW_BC_text_25.txt',\n",
       " 'AW_BC_text_26.txt',\n",
       " 'AW_BC_text_27.txt',\n",
       " 'AW_BC_text_28.txt',\n",
       " 'AW_BC_text_29.txt',\n",
       " 'AW_BC_text_30.txt',\n",
       " 'AW_BC_text_31.txt',\n",
       " 'AW_BC_text_32.txt',\n",
       " 'AW_BC_text_33.txt',\n",
       " 'CP_OTG_text_01.txt',\n",
       " 'CP_OTG_text_02.txt',\n",
       " 'CP_OTG_text_03.txt',\n",
       " 'CP_OTG_text_05.txt',\n",
       " 'CP_OTG_text_06.txt',\n",
       " 'CP_OTG_text_07.txt',\n",
       " 'CP_OTG_text_08.txt',\n",
       " 'CP_OTG_text_09.txt',\n",
       " 'CP_OTG_text_10.txt',\n",
       " 'CP_OTG_text_11.txt',\n",
       " 'CP_OTG_text_12.txt',\n",
       " 'CP_OTG_text_13.txt',\n",
       " 'CP_OTG_text_14.txt',\n",
       " 'CP_OTG_text_15.txt',\n",
       " 'CP_OTG_text_16.txt',\n",
       " 'CP_OTG_text_17.txt',\n",
       " 'CP_OTG_text_18.txt',\n",
       " 'CP_OTG_text_19.txt',\n",
       " 'CP_OTG_text_20.txt',\n",
       " 'CP_OTG_text_21.txt',\n",
       " 'CP_OTG_text_22.txt',\n",
       " 'CP_OTG_text_23.txt',\n",
       " 'DG_W_text_01.txt',\n",
       " 'DG_W_text_02.txt',\n",
       " 'DG_W_text_03.txt',\n",
       " 'DG_W_text_04.txt',\n",
       " 'DG_W_text_05.txt',\n",
       " 'DG_W_text_06.txt',\n",
       " 'DG_W_text_07.txt',\n",
       " 'DG_W_text_08.txt',\n",
       " 'DG_W_text_09.txt',\n",
       " 'DG_W_text_10.txt',\n",
       " 'DG_W_text_11.txt',\n",
       " 'DG_W_text_12.txt',\n",
       " 'DG_W_text_13.txt',\n",
       " 'DG_W_text_14.txt',\n",
       " 'DG_W_text_15.txt',\n",
       " 'DG_W_text_16.txt',\n",
       " 'DG_W_text_17.txt',\n",
       " 'DG_W_text_18.txt',\n",
       " 'DG_W_text_19.txt',\n",
       " 'DG_W_text_20.txt',\n",
       " 'DG_W_text_21.txt',\n",
       " 'DG_W_text_22.txt',\n",
       " 'DG_W_text_23.txt',\n",
       " 'HM_HK_text_01.txt',\n",
       " 'HM_HK_text_02.txt',\n",
       " 'HM_HK_text_03.txt',\n",
       " 'HM_HK_text_04.txt',\n",
       " 'HM_HK_text_05.txt',\n",
       " 'HM_HK_text_06.txt',\n",
       " 'HM_HK_text_07.txt',\n",
       " 'HM_HK_text_08.txt',\n",
       " 'HM_HK_text_09.txt',\n",
       " 'HM_HK_text_10.txt',\n",
       " 'HM_HK_text_11.txt',\n",
       " 'HM_HK_text_12.txt',\n",
       " 'HM_HK_text_13.txt',\n",
       " 'HM_HK_text_14.txt',\n",
       " 'HM_HK_text_15.txt',\n",
       " 'HM_HK_text_16.txt',\n",
       " 'HM_HK_text_17.txt',\n",
       " 'HM_HK_text_18.txt',\n",
       " 'HM_HK_text_19.txt',\n",
       " 'HM_HK_text_20.txt',\n",
       " 'HM_HK_text_21.txt',\n",
       " 'HM_HK_text_22.txt',\n",
       " 'HM_HK_text_23.txt',\n",
       " 'HM_HK_text_24.txt',\n",
       " 'HM_HK_text_25.txt',\n",
       " 'HM_HK_text_26.txt',\n",
       " 'HM_HK_text_27.txt',\n",
       " 'HM_HK_text_28.txt',\n",
       " 'HM_HK_text_29.txt',\n",
       " 'HM_HK_text_30.txt',\n",
       " 'HM_HK_text_31.txt',\n",
       " 'HM_HK_text_32.txt',\n",
       " 'HM_HK_text_33.txt',\n",
       " 'HM_HK_text_34.txt',\n",
       " 'HM_HK_text_35.txt',\n",
       " 'HM_HK_text_36.txt',\n",
       " 'HM_HK_text_37.txt',\n",
       " 'HM_HK_text_38.txt',\n",
       " 'HM_HK_text_39.txt',\n",
       " 'HM_HK_text_40.txt',\n",
       " 'HM_HK_text_41.txt',\n",
       " 'IS_EM_text_01.txt',\n",
       " 'IS_EM_text_02.txt',\n",
       " 'IS_EM_text_03.txt',\n",
       " 'IS_EM_text_04.txt',\n",
       " 'IS_EM_text_05.txt',\n",
       " 'IS_EM_text_06.txt',\n",
       " 'IS_EM_text_07.txt',\n",
       " 'IS_EM_text_08.txt',\n",
       " 'IS_EM_text_09.txt',\n",
       " 'IS_EM_text_10.txt',\n",
       " 'IS_EM_text_11.txt',\n",
       " 'IS_EM_text_12.txt',\n",
       " 'IS_EM_text_13.txt',\n",
       " 'IS_EM_text_14.txt',\n",
       " 'IS_EM_text_15.txt',\n",
       " 'IS_EM_text_16.txt',\n",
       " 'IS_EM_text_17.txt',\n",
       " 'IS_EM_text_18.txt',\n",
       " 'IS_EM_text_19.txt',\n",
       " 'IS_EM_text_20.txt',\n",
       " 'IS_EM_text_21.txt',\n",
       " 'IS_EM_text_22.txt',\n",
       " 'IS_EM_text_23.txt',\n",
       " 'IS_EM_text_24.txt',\n",
       " 'IS_EM_text_25.txt',\n",
       " 'IS_EM_text_26.txt',\n",
       " 'JB_KGRC_text_01.txt',\n",
       " 'JB_KGRC_text_02.txt',\n",
       " 'JB_KGRC_text_03.txt',\n",
       " 'JB_KGRC_text_04.txt',\n",
       " 'JB_KGRC_text_05.txt',\n",
       " 'JB_KGRC_text_06.txt',\n",
       " 'JB_KGRC_text_07.txt',\n",
       " 'JB_KGRC_text_08.txt',\n",
       " 'JB_KGRC_text_09.txt',\n",
       " 'JB_KGRC_text_10.txt',\n",
       " 'JB_KGRC_text_11.txt',\n",
       " 'JB_KGRC_text_12.txt',\n",
       " 'JB_KGRC_text_13.txt',\n",
       " 'JB_KGRC_text_14.txt',\n",
       " 'JB_KGRC_text_15.txt',\n",
       " 'JB_KGRC_text_16.txt',\n",
       " 'JB_KGRC_text_17.txt',\n",
       " 'JB_KGRC_text_18.txt',\n",
       " 'JB_KGRC_text_19.txt',\n",
       " 'JB_KGRC_text_20.txt',\n",
       " 'JB_KGRC_text_21.txt',\n",
       " 'JB_KGRC_text_22.txt',\n",
       " 'JB_KGRC_text_23.txt',\n",
       " 'JB_KGRC_text_24.txt',\n",
       " 'JB_KGRC_text_25.txt',\n",
       " 'JB_KGRC_text_26.txt',\n",
       " 'JB_KGRC_text_27.txt',\n",
       " 'JB_KGRC_text_28.txt',\n",
       " 'JB_KGRC_text_29.txt',\n",
       " 'JG_QT_text_01.txt',\n",
       " 'JG_QT_text_02.txt',\n",
       " 'JG_QT_text_03.txt',\n",
       " 'JG_QT_text_04.txt',\n",
       " 'JG_QT_text_05.txt',\n",
       " 'JG_QT_text_06.txt',\n",
       " 'JG_QT_text_07.txt',\n",
       " 'JG_QT_text_08.txt',\n",
       " 'JG_QT_text_09.txt',\n",
       " 'JG_QT_text_10.txt',\n",
       " 'JG_QT_text_11.txt',\n",
       " 'JG_QT_text_12.txt',\n",
       " 'JG_QT_text_13.txt',\n",
       " 'JG_QT_text_14.txt',\n",
       " 'JG_QT_text_15.txt',\n",
       " 'JG_QT_text_16.txt',\n",
       " 'JG_QT_text_17.txt',\n",
       " 'JG_QT_text_18.txt',\n",
       " 'JG_QT_text_19.txt',\n",
       " 'JG_QT_text_20.txt',\n",
       " 'JG_QT_text_21.txt',\n",
       " 'JG_QT_text_22.txt',\n",
       " 'JG_QT_text_23.txt',\n",
       " 'JG_QT_text_24.txt',\n",
       " 'JG_QT_text_25.txt',\n",
       " 'JG_QT_text_26.txt',\n",
       " 'JG_QT_text_27.txt',\n",
       " 'JG_QT_text_28.txt',\n",
       " 'JG_QT_text_29.txt',\n",
       " 'JG_QT_text_30.txt',\n",
       " 'JG_QT_text_31.txt',\n",
       " 'JG_QT_text_32.txt',\n",
       " 'JG_QT_text_33.txt',\n",
       " 'JG_QT_text_34.txt',\n",
       " 'JG_QT_text_35.txt',\n",
       " 'JG_QT_text_36.txt',\n",
       " 'JG_QT_text_37.txt',\n",
       " 'JG_QT_text_38.txt',\n",
       " 'JG_QT_text_39.txt',\n",
       " 'JG_QT_text_40.txt',\n",
       " 'JG_QT_text_41.txt',\n",
       " 'JL_IHM_text_01.txt',\n",
       " 'JL_IHM_text_02.txt',\n",
       " 'JL_IHM_text_03.txt',\n",
       " 'JL_IHM_text_04.txt',\n",
       " 'JL_IHM_text_05.txt',\n",
       " 'JL_IHM_text_06.txt',\n",
       " 'JL_IHM_text_07.txt',\n",
       " 'JL_IHM_text_08.txt',\n",
       " 'JL_IHM_text_09.txt',\n",
       " 'JL_IHM_text_10.txt',\n",
       " 'JL_IHM_text_11.txt',\n",
       " 'JL_IHM_text_12.txt',\n",
       " 'JL_IHM_text_13.txt',\n",
       " 'JL_IHM_text_14.txt',\n",
       " 'JL_IHM_text_15.txt',\n",
       " 'JL_IHM_text_16.txt',\n",
       " 'JL_IHM_text_17.txt',\n",
       " 'JL_IHM_text_18.txt',\n",
       " 'JL_IHM_text_19.txt',\n",
       " 'JL_IHM_text_20.txt',\n",
       " 'JL_IHM_text_21.txt',\n",
       " 'JL_IHM_text_22.txt',\n",
       " 'JY_GD_text_01.txt',\n",
       " 'JY_GD_text_02.txt',\n",
       " 'JY_GD_text_03.txt',\n",
       " 'JY_GD_text_04.txt',\n",
       " 'JY_GD_text_05.txt',\n",
       " 'JY_GD_text_06.txt',\n",
       " 'JY_GD_text_07.txt',\n",
       " 'JY_GD_text_08.txt',\n",
       " 'JY_GD_text_09.txt',\n",
       " 'JY_GD_text_10.txt',\n",
       " 'JY_GD_text_11.txt',\n",
       " 'JY_GD_text_12.txt',\n",
       " 'JY_GD_text_13.txt',\n",
       " 'JY_GD_text_14.txt',\n",
       " 'JY_GD_text_15.txt',\n",
       " 'JY_GD_text_16.txt',\n",
       " 'JY_GD_text_17.txt',\n",
       " 'JY_GD_text_18.txt',\n",
       " 'JY_GD_text_19.txt',\n",
       " 'JY_GD_text_20.txt',\n",
       " 'JY_GD_text_21.txt',\n",
       " 'JY_GD_text_22.txt',\n",
       " 'JY_GD_text_23.txt',\n",
       " 'JY_GD_text_24.txt',\n",
       " 'JY_GD_text_25.txt',\n",
       " 'JY_GD_text_26.txt',\n",
       " 'JY_GD_text_27.txt',\n",
       " 'JY_GD_text_28.txt',\n",
       " 'JY_GD_text_29.txt',\n",
       " 'JY_GD_text_30.txt',\n",
       " 'JY_GD_text_31.txt',\n",
       " 'JY_GD_text_32.txt',\n",
       " 'LCK_SY_text_01.txt',\n",
       " 'LCK_SY_text_02.txt',\n",
       " 'LCK_SY_text_03.txt',\n",
       " 'LCK_SY_text_04.txt',\n",
       " 'LCK_SY_text_05.txt',\n",
       " 'LCK_SY_text_06.txt',\n",
       " 'LCK_SY_text_07.txt',\n",
       " 'LCK_SY_text_08.txt',\n",
       " 'LCK_SY_text_09.txt',\n",
       " 'LCK_SY_text_10.txt',\n",
       " 'LCK_SY_text_11.txt',\n",
       " 'LCK_SY_text_12.txt',\n",
       " 'LCK_SY_text_13.txt',\n",
       " 'LCK_SY_text_14.txt',\n",
       " 'LCK_SY_text_15.txt',\n",
       " 'LCK_SY_text_16.txt',\n",
       " 'LCK_SY_text_17.txt',\n",
       " 'LCK_SY_text_18.txt',\n",
       " 'LCK_SY_text_19.txt',\n",
       " 'LCK_SY_text_20.txt',\n",
       " 'LCK_SY_text_21.txt',\n",
       " 'LCK_SY_text_22.txt',\n",
       " 'LCK_SY_text_23.txt',\n",
       " 'LCK_SY_text_24.txt',\n",
       " 'LCK_SY_text_25.txt',\n",
       " 'LCK_SY_text_26.txt',\n",
       " 'LCK_SY_text_27.txt',\n",
       " 'LCK_SY_text_28.txt',\n",
       " 'LCK_SY_text_29.txt',\n",
       " 'LCK_SY_text_30.txt',\n",
       " 'LCK_SY_text_31.txt',\n",
       " 'LCK_SY_text_32.txt',\n",
       " 'LCK_SY_text_33.txt',\n",
       " 'LCK_SY_text_34.txt',\n",
       " 'my_model.h5',\n",
       " 'my_model.keras',\n",
       " 'NB_TGAA_text_01.txt',\n",
       " 'NB_TGAA_text_02.txt',\n",
       " 'NB_TGAA_text_03.txt',\n",
       " 'NB_TGAA_text_04.txt',\n",
       " 'NB_TGAA_text_05.txt',\n",
       " 'NB_TGAA_text_06.txt',\n",
       " 'NB_TGAA_text_07.txt',\n",
       " 'NB_TGAA_text_08.txt',\n",
       " 'NB_TGAA_text_09.txt',\n",
       " 'NB_TGAA_text_10.txt',\n",
       " 'NB_TGAA_text_11.txt',\n",
       " 'NB_TGAA_text_12.txt',\n",
       " 'NB_TGAA_text_13.txt',\n",
       " 'NB_TGAA_text_14.txt',\n",
       " 'NB_TGAA_text_15.txt',\n",
       " 'NB_TGAA_text_16.txt',\n",
       " 'NB_TGAA_text_17.txt',\n",
       " 'NB_TGAA_text_18.txt',\n",
       " 'NB_TGAA_text_20.txt',\n",
       " 'NB_TGAA_text_21.txt',\n",
       " 'NB_TGAA_text_22.txt',\n",
       " 'NB_TGAA_text_23.txt',\n",
       " 'NB_TK_text_01.txt',\n",
       " 'NB_TK_text_02.txt',\n",
       " 'NB_TK_text_03.txt',\n",
       " 'NB_TK_text_04.txt',\n",
       " 'NB_TK_text_05.txt',\n",
       " 'NB_TK_text_06.txt',\n",
       " 'NB_TK_text_07.txt',\n",
       " 'NB_TK_text_08.txt',\n",
       " 'NB_TK_text_09.txt',\n",
       " 'NB_TK_text_10.txt',\n",
       " 'NB_TK_text_11.txt',\n",
       " 'NB_TK_text_12.txt',\n",
       " 'NB_TK_text_13.txt',\n",
       " 'NB_TK_text_14.txt',\n",
       " 'NB_TK_text_15.txt',\n",
       " 'NB_TK_text_16.txt',\n",
       " 'NB_TK_text_17.txt',\n",
       " 'NB_TK_text_18.txt',\n",
       " 'NB_TK_text_19.txt',\n",
       " 'NB_TK_text_20.txt',\n",
       " 'NB_TK_text_21.txt',\n",
       " 'NB_TK_text_22.txt',\n",
       " 'NB_TK_text_23.txt',\n",
       " 'NB_TK_text_24.txt',\n",
       " 'NB_TK_text_25.txt',\n",
       " 'NB_TK_text_26.txt',\n",
       " 'NB_TK_text_27.txt',\n",
       " 'NB_TK_text_28.txt',\n",
       " 'NB_TK_text_29.txt',\n",
       " 'NB_TK_text_30.txt',\n",
       " 'NB_TK_text_31.txt',\n",
       " 'NB_TK_text_32.txt',\n",
       " 'NB_TK_text_33.txt',\n",
       " 'NB_TK_text_34.txt',\n",
       " 'NB_TK_text_35.txt',\n",
       " 'RP_DP_text_01.txt',\n",
       " 'RP_DP_text_02.txt',\n",
       " 'RP_DP_text_03.txt',\n",
       " 'RP_DP_text_04.txt',\n",
       " 'RP_DP_text_05.txt',\n",
       " 'RP_DP_text_06.txt',\n",
       " 'RP_DP_text_08.txt',\n",
       " 'RP_DP_text_09.txt',\n",
       " 'RP_DP_text_10.txt',\n",
       " 'RP_DP_text_11.txt',\n",
       " 'RP_DP_text_12.txt',\n",
       " 'RP_DP_text_13.txt',\n",
       " 'RP_DP_text_14.txt',\n",
       " 'RP_DP_text_15.txt',\n",
       " 'RP_DP_text_16.txt',\n",
       " 'RP_DP_text_17.txt',\n",
       " 'RP_DP_text_18.txt',\n",
       " 'RP_DP_text_19.txt',\n",
       " 'RP_DP_text_20.txt',\n",
       " 'RP_DP_text_21.txt',\n",
       " 'RP_DP_text_22.txt',\n",
       " 'RP_DP_text_23.txt',\n",
       " 'RP_DP_text_24.txt',\n",
       " 'RP_DP_text_25.txt',\n",
       " 'RP_DP_text_26.txt',\n",
       " 'RP_DP_text_27.txt',\n",
       " 'RP_DP_text_28.txt',\n",
       " 'RP_DP_text_29.txt',\n",
       " 'RP_DP_text_30.txt',\n",
       " 'RP_DP_text_31.txt',\n",
       " 'RP_DP_text_32.txt',\n",
       " 'RP_DP_text_33.txt',\n",
       " 'RP_DP_text_34.txt',\n",
       " 'RP_DP_text_35.txt',\n",
       " 'RP_DP_text_36.txt',\n",
       " 'SM_IGT_text_01.txt',\n",
       " 'SM_IGT_text_02.txt',\n",
       " 'SM_IGT_text_03.txt',\n",
       " 'SM_IGT_text_04.txt',\n",
       " 'SM_IGT_text_05.txt',\n",
       " 'SM_IGT_text_06.txt',\n",
       " 'SM_IGT_text_07.txt',\n",
       " 'SM_IGT_text_08.txt',\n",
       " 'SM_IGT_text_09.txt',\n",
       " 'SM_IGT_text_10.txt',\n",
       " 'SM_IGT_text_11.txt',\n",
       " 'SM_IGT_text_12.txt',\n",
       " 'SM_IGT_text_13.txt',\n",
       " 'SM_IGT_text_14.txt',\n",
       " 'SM_IGT_text_15.txt',\n",
       " 'SM_IGT_text_16.txt',\n",
       " 'SM_IGT_text_17.txt',\n",
       " 'SM_IGT_text_18.txt',\n",
       " 'SM_IGT_text_19.txt',\n",
       " 'SM_IGT_text_20.txt',\n",
       " 'TN_AD_text_01.txt',\n",
       " 'TN_AD_text_02.txt',\n",
       " 'TN_AD_text_03.txt',\n",
       " 'TN_AD_text_04.txt',\n",
       " 'TN_AD_text_06.txt',\n",
       " 'TN_AD_text_07.txt',\n",
       " 'TN_AD_text_08.txt',\n",
       " 'TN_AD_text_09.txt',\n",
       " 'TN_AD_text_10.txt',\n",
       " 'TN_AD_text_11.txt',\n",
       " 'TN_AD_text_12.txt',\n",
       " 'TN_AD_text_13.txt',\n",
       " 'TN_AD_text_14.txt',\n",
       " 'TN_AD_text_15.txt',\n",
       " 'TN_AD_text_16.txt',\n",
       " 'TN_AD_text_17.txt',\n",
       " 'TN_AD_text_18.txt',\n",
       " 'TN_AD_text_19.txt',\n",
       " 'TS_D_text_01.txt',\n",
       " 'TS_D_text_02.txt',\n",
       " 'TS_D_text_03.txt',\n",
       " 'TS_D_text_04.txt',\n",
       " 'TS_D_text_05.txt',\n",
       " 'TS_D_text_06.txt',\n",
       " 'TS_D_text_07.txt',\n",
       " 'TS_D_text_08.txt',\n",
       " 'TS_D_text_09.txt',\n",
       " 'TS_D_text_10.txt',\n",
       " 'TS_D_text_11.txt',\n",
       " 'TS_D_text_12.txt',\n",
       " 'TS_D_text_13.txt',\n",
       " 'TS_D_text_14.txt',\n",
       " 'TS_D_text_15.txt',\n",
       " 'TS_D_text_16.txt',\n",
       " 'TS_D_text_17.txt',\n",
       " 'TS_D_text_18.txt',\n",
       " 'TS_D_text_19.txt',\n",
       " 'TS_D_text_20.txt',\n",
       " 'TS_D_text_21.txt',\n",
       " 'TS_D_text_22.txt',\n",
       " 'TS_D_text_23.txt',\n",
       " 'TS_D_text_24.txt',\n",
       " 'TS_D_text_25.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all the data from each text file\n",
    "path = 'C:/Users/geniu/Comedic line generator/funny_text'\n",
    "os.chdir(path)\n",
    "loaded_data = \"\"\n",
    "\n",
    "for file in os.listdir():\n",
    "    file_path = os.path.join(path, file)  # Join the folder path with the file name\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        loaded_data = \" \".join([loaded_data, text])\n",
    "\n",
    "print(loaded_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e6881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CLEANING THE DATA TO REDUCE VOCABULARY SIZE\n",
    "\n",
    "import string\n",
    "\n",
    "# turn the data into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '-' with a space ' '\n",
    "    doc = doc.replace('-', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case(to lessen the vocab size)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f76ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'thank', 'you', 'thank', 'you', 'san', 'francisco', 'thank', 'you', 'so', 'much', 'so', 'good', 'to', 'be', 'here', 'people', 'were', 'surprised', 'when', 'i', 'told', 'i', 'was', 'gonna', 'tape', 'my', 'special', 'in', 'san', 'francisco', 'said', 'would', 'you', 'do', 'that', 'the', 'most', 'politically', 'correct', 'city', 'in', 'the', 'not', 'when', 'on', 'stage', 'noti', 'love', 'this', 'place', 'one', 'of', 'my', 'favorite', 'cities', 'to', 'perform', 'in', 'a', 'good', 'place', 'to', 'hang', 'out', 'got', 'to', 'walk', 'around', 'a', 'little', 'bit', 'today', 'saw', 'a', 'baby', 'saw', 'a', 'baby', 'locked', 'inside', 'the', 'back', 'of', 'a', 'hot', 'car', 'so', 'been', 'a', 'great', 'day', 'love', 'that', 'get', 'me', 'wrong', 'not', 'a', 'monster', 'i', 'tried', 'to', 'help', 'the', 'baby', 'tried', 'to', 'throw', 'a', 'rock', 'through', 'the', 'window', 'window', 'was', 'down', 'ruined', 'that', 'whole', 'weekend', 'it', 'was', 'worth', 'it', 'i', 'love', 'san', 'francisco', 'one', 'of', 'my', 'favorite', 'things', 'is', 'how', 'beautiful', 'all', 'the', 'women', 'are', 'here', 'yeah', 'women', 'in', 'san', 'francisco', 'are', 'gorgeous', 'i', 'say', 'that', 'despite', 'going', 'on', 'in', 'this', 'crowd', 'tonight', 'but', 'you', 'guys', 'live', 'here', 'you', 'guys', 'live', 'here', 'seen', 'i', 'was', 'in', 'a', 'bar', 'last', 'night', 'saw', 'this', 'beautiful', 'woman', 'like', 'a', 'supermodel', 'i', 'walked', 'up', 'i', 'was', 'like', 'where', 'you', 'from', 'what', 'do', 'you', 'she', 'goes', 'me', 'i']\n",
      "Total Tokens: 126946\n",
      "Unique Tokens: 7894\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(loaded_data)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens)) #number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df071dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 126895\n"
     ]
    }
   ],
   "source": [
    "#logic : a sequence of predefined length(50,here) is used to predict the next word\n",
    "## organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d4c5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one sdequence per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b54b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving it(till here, the steps are already done , you need to unzip the file named 'comedic_sequences' ! )\n",
    "filename = \"C:/Users/geniu/Comedic line generator/comedic_sequences.txt\"\n",
    "save_doc(sequences,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf726e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you thank you thank you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this', 'you thank you thank you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place', 'thank you thank you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one', 'you thank you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of', 'thank you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my', 'you san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite', 'san francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities', 'francisco thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to', 'thank you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform', 'you so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in', 'so much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a', 'much so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good', 'so good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place', 'good to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to', 'to be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang', 'be here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang out', 'here people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang out got', 'people were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang out got to', 'were surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang out got to walk', 'surprised when i told i was gonna tape my special in san francisco said would you do that the most politically correct city in the not when on stage noti love this place one of my favorite cities to perform in a good place to hang out got to walk around']\n"
     ]
    }
   ],
   "source": [
    "#printing first few lines of the sequence \n",
    "with open(\"C:/Users/geniu/Comedic line generator/comedic_sequences.txt\", 'r') as f2:\n",
    "        t2 = f2.read()\n",
    "        lines = t2.split('\\n')\n",
    "        \n",
    "print(lines[:20])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "129e0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46e219a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3effa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61e3e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)  #encoding the output words for each input-output sequence pair.\n",
    "seq_length = X.shape[1]    #shape remains generic inthe sense we can change the length of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2f3e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 50, 50)            394750    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7895)              797395    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1343045 (5.12 MB)\n",
      "Trainable params: 1343045 (5.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "282d855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "992/992 [==============================] - 225s 220ms/step - loss: 6.2780 - accuracy: 0.0451\n",
      "Epoch 2/100\n",
      "992/992 [==============================] - 207s 209ms/step - loss: 5.8790 - accuracy: 0.0566\n",
      "Epoch 3/100\n",
      "992/992 [==============================] - 204s 206ms/step - loss: 5.6706 - accuracy: 0.0809\n",
      "Epoch 4/100\n",
      "992/992 [==============================] - 200s 202ms/step - loss: 5.5113 - accuracy: 0.1019\n",
      "Epoch 5/100\n",
      "992/992 [==============================] - 199s 200ms/step - loss: 5.3796 - accuracy: 0.1147\n",
      "Epoch 6/100\n",
      "992/992 [==============================] - 206s 207ms/step - loss: 5.2788 - accuracy: 0.1226\n",
      "Epoch 7/100\n",
      "992/992 [==============================] - 177s 178ms/step - loss: 5.2025 - accuracy: 0.1280\n",
      "Epoch 8/100\n",
      "992/992 [==============================] - 181s 182ms/step - loss: 5.1407 - accuracy: 0.1307\n",
      "Epoch 9/100\n",
      "992/992 [==============================] - 177s 178ms/step - loss: 5.0667 - accuracy: 0.1353\n",
      "Epoch 10/100\n",
      "992/992 [==============================] - 177s 179ms/step - loss: 4.9971 - accuracy: 0.1384\n",
      "Epoch 11/100\n",
      "992/992 [==============================] - 178s 179ms/step - loss: 4.9291 - accuracy: 0.1410\n",
      "Epoch 12/100\n",
      "992/992 [==============================] - 180s 181ms/step - loss: 4.8513 - accuracy: 0.1441\n",
      "Epoch 13/100\n",
      "992/992 [==============================] - 200s 201ms/step - loss: 4.7935 - accuracy: 0.1472\n",
      "Epoch 14/100\n",
      "992/992 [==============================] - 191s 192ms/step - loss: 4.7425 - accuracy: 0.1500\n",
      "Epoch 15/100\n",
      "992/992 [==============================] - 181s 183ms/step - loss: 4.6755 - accuracy: 0.1525\n",
      "Epoch 16/100\n",
      "992/992 [==============================] - 173s 174ms/step - loss: 4.6930 - accuracy: 0.1518\n",
      "Epoch 17/100\n",
      "992/992 [==============================] - 185s 187ms/step - loss: 4.6443 - accuracy: 0.1533\n",
      "Epoch 18/100\n",
      "992/992 [==============================] - 168s 170ms/step - loss: 4.5841 - accuracy: 0.1570\n",
      "Epoch 19/100\n",
      "992/992 [==============================] - 174s 175ms/step - loss: 4.5297 - accuracy: 0.1596\n",
      "Epoch 20/100\n",
      "992/992 [==============================] - 177s 178ms/step - loss: 4.4783 - accuracy: 0.1623\n",
      "Epoch 21/100\n",
      "992/992 [==============================] - 173s 175ms/step - loss: 4.4298 - accuracy: 0.1655\n",
      "Epoch 22/100\n",
      "992/992 [==============================] - 184s 186ms/step - loss: 4.3811 - accuracy: 0.1690\n",
      "Epoch 23/100\n",
      "992/992 [==============================] - 165s 166ms/step - loss: 4.3332 - accuracy: 0.1718\n",
      "Epoch 24/100\n",
      "992/992 [==============================] - 171s 172ms/step - loss: 4.2866 - accuracy: 0.1753\n",
      "Epoch 25/100\n",
      "992/992 [==============================] - 218s 220ms/step - loss: 4.2400 - accuracy: 0.1791\n",
      "Epoch 26/100\n",
      "992/992 [==============================] - 220s 221ms/step - loss: 4.1954 - accuracy: 0.1824\n",
      "Epoch 27/100\n",
      "992/992 [==============================] - 217s 219ms/step - loss: 4.1552 - accuracy: 0.1865\n",
      "Epoch 28/100\n",
      "992/992 [==============================] - 220s 221ms/step - loss: 4.1118 - accuracy: 0.1903\n",
      "Epoch 29/100\n",
      "992/992 [==============================] - 218s 220ms/step - loss: 4.0701 - accuracy: 0.1940\n",
      "Epoch 30/100\n",
      "992/992 [==============================] - 218s 220ms/step - loss: 4.0317 - accuracy: 0.1980\n",
      "Epoch 31/100\n",
      "992/992 [==============================] - 218s 220ms/step - loss: 3.9933 - accuracy: 0.2023\n",
      "Epoch 32/100\n",
      "992/992 [==============================] - 206s 207ms/step - loss: 3.9560 - accuracy: 0.2054\n",
      "Epoch 33/100\n",
      "992/992 [==============================] - 226s 227ms/step - loss: 3.9205 - accuracy: 0.2093\n",
      "Epoch 34/100\n",
      "992/992 [==============================] - 263s 266ms/step - loss: 3.8857 - accuracy: 0.2138\n",
      "Epoch 35/100\n",
      "992/992 [==============================] - 222s 224ms/step - loss: 3.8507 - accuracy: 0.2179\n",
      "Epoch 36/100\n",
      "992/992 [==============================] - 222s 224ms/step - loss: 3.8187 - accuracy: 0.2225\n",
      "Epoch 37/100\n",
      "992/992 [==============================] - 162s 163ms/step - loss: 3.7841 - accuracy: 0.2260\n",
      "Epoch 38/100\n",
      "992/992 [==============================] - 162s 163ms/step - loss: 3.7511 - accuracy: 0.2296\n",
      "Epoch 39/100\n",
      "992/992 [==============================] - 188s 190ms/step - loss: 3.7240 - accuracy: 0.2330\n",
      "Epoch 40/100\n",
      "992/992 [==============================] - 223s 225ms/step - loss: 3.6885 - accuracy: 0.2372\n",
      "Epoch 41/100\n",
      "992/992 [==============================] - 217s 219ms/step - loss: 3.6587 - accuracy: 0.2413\n",
      "Epoch 42/100\n",
      "992/992 [==============================] - 215s 217ms/step - loss: 3.6310 - accuracy: 0.2458\n",
      "Epoch 43/100\n",
      "992/992 [==============================] - 220s 222ms/step - loss: 3.5977 - accuracy: 0.2493\n",
      "Epoch 44/100\n",
      "992/992 [==============================] - 216s 217ms/step - loss: 3.5691 - accuracy: 0.2544\n",
      "Epoch 45/100\n",
      "992/992 [==============================] - 203s 205ms/step - loss: 3.5416 - accuracy: 0.2572\n",
      "Epoch 46/100\n",
      "992/992 [==============================] - 187s 188ms/step - loss: 3.5134 - accuracy: 0.2613\n",
      "Epoch 47/100\n",
      "992/992 [==============================] - 171s 173ms/step - loss: 3.4848 - accuracy: 0.2668\n",
      "Epoch 48/100\n",
      "992/992 [==============================] - 201s 203ms/step - loss: 3.4565 - accuracy: 0.2704\n",
      "Epoch 49/100\n",
      "992/992 [==============================] - 188s 189ms/step - loss: 3.4330 - accuracy: 0.2726\n",
      "Epoch 50/100\n",
      "992/992 [==============================] - 229s 231ms/step - loss: 3.4042 - accuracy: 0.2780\n",
      "Epoch 51/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 3.3752 - accuracy: 0.2821\n",
      "Epoch 52/100\n",
      "992/992 [==============================] - 255s 257ms/step - loss: 3.3488 - accuracy: 0.2861\n",
      "Epoch 53/100\n",
      "992/992 [==============================] - 256s 258ms/step - loss: 3.3249 - accuracy: 0.2898\n",
      "Epoch 54/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 3.2989 - accuracy: 0.2933\n",
      "Epoch 55/100\n",
      "992/992 [==============================] - 288s 290ms/step - loss: 3.2744 - accuracy: 0.2974\n",
      "Epoch 56/100\n",
      "992/992 [==============================] - 293s 295ms/step - loss: 3.2493 - accuracy: 0.3008\n",
      "Epoch 57/100\n",
      "992/992 [==============================] - 297s 299ms/step - loss: 3.2239 - accuracy: 0.3048\n",
      "Epoch 58/100\n",
      "992/992 [==============================] - 296s 298ms/step - loss: 3.2016 - accuracy: 0.3084\n",
      "Epoch 59/100\n",
      "992/992 [==============================] - 294s 297ms/step - loss: 3.1753 - accuracy: 0.3126\n",
      "Epoch 60/100\n",
      "992/992 [==============================] - 290s 293ms/step - loss: 3.1516 - accuracy: 0.3165\n",
      "Epoch 61/100\n",
      "992/992 [==============================] - 351s 354ms/step - loss: 3.1270 - accuracy: 0.3207\n",
      "Epoch 62/100\n",
      "992/992 [==============================] - 236s 238ms/step - loss: 3.1047 - accuracy: 0.3238\n",
      "Epoch 63/100\n",
      "992/992 [==============================] - 190s 191ms/step - loss: 3.0826 - accuracy: 0.3273\n",
      "Epoch 64/100\n",
      "992/992 [==============================] - 190s 191ms/step - loss: 3.0606 - accuracy: 0.3310\n",
      "Epoch 65/100\n",
      "992/992 [==============================] - 188s 189ms/step - loss: 3.0363 - accuracy: 0.3336\n",
      "Epoch 66/100\n",
      "992/992 [==============================] - 229s 231ms/step - loss: 3.0144 - accuracy: 0.3388\n",
      "Epoch 67/100\n",
      "992/992 [==============================] - 276s 279ms/step - loss: 2.9921 - accuracy: 0.3418\n",
      "Epoch 68/100\n",
      "992/992 [==============================] - 334s 336ms/step - loss: 2.9698 - accuracy: 0.3464\n",
      "Epoch 69/100\n",
      "992/992 [==============================] - 332s 335ms/step - loss: 2.9494 - accuracy: 0.3496\n",
      "Epoch 70/100\n",
      "992/992 [==============================] - 324s 326ms/step - loss: 2.9280 - accuracy: 0.3536\n",
      "Epoch 71/100\n",
      "992/992 [==============================] - 337s 339ms/step - loss: 2.9090 - accuracy: 0.3561\n",
      "Epoch 72/100\n",
      "992/992 [==============================] - 248s 249ms/step - loss: 2.8908 - accuracy: 0.3585\n",
      "Epoch 73/100\n",
      "992/992 [==============================] - 231s 233ms/step - loss: 2.8687 - accuracy: 0.3621\n",
      "Epoch 74/100\n",
      "992/992 [==============================] - 213s 215ms/step - loss: 2.8465 - accuracy: 0.3666\n",
      "Epoch 75/100\n",
      "992/992 [==============================] - 260s 262ms/step - loss: 2.8248 - accuracy: 0.3704\n",
      "Epoch 76/100\n",
      "992/992 [==============================] - 1833s 2s/step - loss: 2.8095 - accuracy: 0.3726\n",
      "Epoch 77/100\n",
      "992/992 [==============================] - 326s 329ms/step - loss: 2.7879 - accuracy: 0.3767\n",
      "Epoch 78/100\n",
      "992/992 [==============================] - 342s 344ms/step - loss: 2.7683 - accuracy: 0.3806\n",
      "Epoch 79/100\n",
      "992/992 [==============================] - 214s 215ms/step - loss: 2.7521 - accuracy: 0.3829\n",
      "Epoch 80/100\n",
      "992/992 [==============================] - 188s 190ms/step - loss: 2.7279 - accuracy: 0.3881\n",
      "Epoch 81/100\n",
      "992/992 [==============================] - 193s 194ms/step - loss: 2.7103 - accuracy: 0.3914\n",
      "Epoch 82/100\n",
      "992/992 [==============================] - 263s 265ms/step - loss: 2.6922 - accuracy: 0.3939\n",
      "Epoch 83/100\n",
      "992/992 [==============================] - 289s 291ms/step - loss: 2.6753 - accuracy: 0.3977\n",
      "Epoch 84/100\n",
      "992/992 [==============================] - 289s 292ms/step - loss: 2.6592 - accuracy: 0.4001\n",
      "Epoch 85/100\n",
      "992/992 [==============================] - 287s 289ms/step - loss: 2.6405 - accuracy: 0.4032\n",
      "Epoch 86/100\n",
      "992/992 [==============================] - 198s 199ms/step - loss: 2.6246 - accuracy: 0.4059\n",
      "Epoch 87/100\n",
      "992/992 [==============================] - 208s 210ms/step - loss: 2.6047 - accuracy: 0.4107\n",
      "Epoch 88/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 2.5884 - accuracy: 0.4132\n",
      "Epoch 89/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 2.5724 - accuracy: 0.4157\n",
      "Epoch 90/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 2.5540 - accuracy: 0.4194\n",
      "Epoch 91/100\n",
      "992/992 [==============================] - 253s 255ms/step - loss: 2.5388 - accuracy: 0.4221\n",
      "Epoch 92/100\n",
      "992/992 [==============================] - 254s 256ms/step - loss: 2.5240 - accuracy: 0.4243\n",
      "Epoch 93/100\n",
      "992/992 [==============================] - 273s 276ms/step - loss: 2.5070 - accuracy: 0.4269\n",
      "Epoch 94/100\n",
      "992/992 [==============================] - 287s 289ms/step - loss: 2.4925 - accuracy: 0.4298\n",
      "Epoch 95/100\n",
      "992/992 [==============================] - 286s 288ms/step - loss: 2.4780 - accuracy: 0.4328\n",
      "Epoch 96/100\n",
      "992/992 [==============================] - 292s 294ms/step - loss: 2.4550 - accuracy: 0.4371\n",
      "Epoch 97/100\n",
      "992/992 [==============================] - 278s 280ms/step - loss: 2.4438 - accuracy: 0.4396\n",
      "Epoch 98/100\n",
      "992/992 [==============================] - 278s 281ms/step - loss: 2.4338 - accuracy: 0.4419\n",
      "Epoch 99/100\n",
      "992/992 [==============================] - 294s 296ms/step - loss: 2.4129 - accuracy: 0.4457\n",
      "Epoch 100/100\n",
      "992/992 [==============================] - 273s 276ms/step - loss: 2.3977 - accuracy: 0.4482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2483f210130>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26a2bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "127015e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/geniu/Comedic line generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3f48b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'my_model.keras'\n",
    "model.save(model_filename)\n",
    "\n",
    "import joblib\n",
    "# save the tokenizer\n",
    "joblib.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6056be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a521c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # open the file as read only\n",
    "file = open(\"C:/Users/geniu/Comedic line generator/comedic_sequences.txt\", 'r')\n",
    " # read all text\n",
    "text = file.read()\n",
    " # close the file\n",
    "file.close()\n",
    "\n",
    "sample = text.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985ef037",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(sample[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e77dfb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = joblib.load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69d34e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were we where is this going i first thank you i thank you enough for coming out tonight it means the world and been this is how been closing shows on the road and i want to close my special with it assuming that people saw the netflix the standups the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "import random\n",
    "seed_text = sample[random.randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd217668",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cb397a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict(encoded, verbose=0)\n",
    "yhat_indices = np.argmax(yhat_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c7e449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the just just to world are like and to i like to i over the back girls to like like news to audience global world to to global talked been in news to audience like you the that dog to in like to car was but news of news through news\n"
     ]
    }
   ],
   "source": [
    "# Convert the predicted word indices to words\n",
    "out_words = []\n",
    "for index in yhat_indices:\n",
    "    word = tokenizer.index_word.get(index, None)\n",
    "    if word:\n",
    "        out_words.append(word)\n",
    "\n",
    "# Join the list of words to form the generated text\n",
    "generated_text = \" \".join(out_words)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
